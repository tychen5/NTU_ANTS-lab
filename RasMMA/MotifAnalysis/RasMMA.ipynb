{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A implementation of Sequence-Based Behavior Group Clustering Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "% run Alignment_Fast3.ipynb\n",
    "% run StructMatchGap3.ipynb\n",
    "% run StageMatrix.ipynb\n",
    "% run Motif.ipynb\n",
    "% run OutputStage.ipynb\n",
    "% run CommonMotifAnalysis_Tmp.ipynb\n",
    "\n",
    "# Doing global alignment and Calculate common motif.\n",
    "# will return a common motif dict\n",
    "def do_globalAlignment(rep1, rep2):\n",
    "    # Aligment\n",
    "    align_dict = dict()\n",
    "    BASE = \"rep1\"\n",
    "    align_dict['rep1'] = pairwise_NW( rep1, rep1, 2, -1, -3, 1)[2]\n",
    "    align_dict['rep2'] = pairwise_NW( rep1, rep2, 2, -1, -3, 1)[2]\n",
    "    \n",
    "    # get 'Match Matrix' and 'Gap List'\n",
    "    matchMatrix, gapSeqList = structMatchGap(align_dict, BASE)\n",
    "    stageMatrixResult = stageMatrix(matchMatrix, gapSeqList)\n",
    "    Motif_Obj = Motif(stageMatrixResult, BASE)\n",
    "    outputStage = OutputStage(stageMatrixResult, None, BASE, Motif_Obj)\n",
    "    \n",
    "    executionTrace_dict = {\"rep1\":rep1, \"rep2\":rep2}\n",
    "    \n",
    "    commonMotif = CommonMotif(stageMatrixResult, Motif_Obj, executionTrace_dict, outputStage)\n",
    "    \n",
    "    # comMotifdict= {'s<stage>_<motif>': [CMS], oriIdxRange1, oriIdxRange2},\n",
    "    comMotif_dict = commonMotif.getComMotifDict()  \n",
    "    return comMotif_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicateAPI(featureTrace): # remove duplicate api if continuously occur\n",
    "    result = []\n",
    "    lastAPI = \"\"\n",
    "    for api in featureTrace:\n",
    "        if lastAPI != api: # find new api\n",
    "            result.append(api)\n",
    "            lastAPI = api\n",
    "    return result\n",
    "\n",
    "def removeUnwantedAPI(featureTrace): # remove unwanted api\n",
    "    result = []\n",
    "    unwanted_api = {'CloseHandle', 'OpenThread', 'RegOpenKey', 'RegCloseKey'}\n",
    "    frequently_used_lib = {'imm32', 'lpk', 'gdi32', 'kernel32', 'ntdll', 'user32', 'comctl32', 'advapi64'}\n",
    "\n",
    "    for api in featureTrace:\n",
    "        API = api.split('#')[0]\n",
    "        \n",
    "        if API == \"LoadLibrary\": # api is LoadLibrary\n",
    "            libName = api.split(\"@\")[2]\n",
    "            if libName not in frequently_used_lib: # found new library, add it into lib_set and result_Hooklog\n",
    "                result.append(api)\n",
    "                frequently_used_lib.update(libName)\n",
    "                \n",
    "        elif API not in unwanted_api: # api not unwanted\n",
    "            result.append(api)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "% run FeatureTrace.ipynb\n",
    "#******************** the output toMergeCandidate_Dict have to change to set\n",
    "\n",
    "# initialize all traces as \"to merge candidates clusters\"\n",
    "def initialCandidateDict(data_directory):\n",
    "    \n",
    "#     toMergeCandidate_List = [] #list()\n",
    "    toMergeCandidate_Dict = dict()\n",
    "    \n",
    "    # get feature hooklogs\n",
    "    FeatTrace = FeatureTrace\n",
    "    traceName_list = [f for f in os.listdir(data_directory) if f.endswith('.trace.hooklog') ] # py36_leoqaz12\n",
    "    ft_count = 0\n",
    "    for traceName in traceName_list:\n",
    "        featureTrace = FeatTrace(data_directory + traceName).getTrace_noContainTS()\n",
    "#         featureTrace = [line.rstrip('\\n') for line in open(data_directory + traceName)] # use txt as featureTrace directly\n",
    "        featureTrace = removeDuplicateAPI(featureTrace)    \n",
    "        featureTrace = removeUnwantedAPI(featureTrace)\n",
    "        clusterName = \"G\"+str(ft_count)\n",
    "        # R = tuple( clusterName, list(  tuple(featureTrace, fTStartIdx, fTEndIdx) ) ), the representative of cluster.\n",
    "        R = (clusterName, [(featureTrace, 0, len(featureTrace)-1)] )\n",
    "        clusterMembers = set()\n",
    "        traceName = shortenHooklogName(traceName)\n",
    "        clusterMembers.add(traceName)\n",
    "        \n",
    "        toMergeCandidate_Dict[ft_count] = (R, clusterMembers)\n",
    "        \n",
    "        ft_count+=1\n",
    "        \n",
    "#     print(\"-- Finish Initializing --\")\n",
    "    return toMergeCandidate_Dict\n",
    "#     return toMergeCandidateSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten Name to first 6 charactors\n",
    "def shortenHooklogName(traceName):\n",
    "    hashValue = traceName[0:6]\n",
    "    pid = traceName.split(\"_\")[1].split(\".\")[0]\n",
    "    return hashValue+\"_\"+pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: two R\n",
    "# output: new RepresentativeR of inputs;\n",
    "def get_Representative(Ri, Rj):\n",
    "    rep1 = [] #list()\n",
    "    rep2 = [] #list()\n",
    "\n",
    "#     print(Ri[0], Rj[0])\n",
    "    for i in range(len(Ri[1])): # get length of R's common motif seqs  (p.s. Ri[0] is clusterName)\n",
    "        rep1 += Ri[1][i][0]\n",
    "    for i in range(len(Rj[1])):\n",
    "        rep2 += Rj[1][i][0]\n",
    "    \n",
    "    repNew = [] #list() \n",
    "    \n",
    "    if(rep1 and rep2):\n",
    "        comMotif_dict = do_globalAlignment(rep1, rep2) # do Alignment\n",
    "        newStartIdx = 0\n",
    "\n",
    "        for m in sorted(comMotif_dict.keys(), key = lambda x : int(x.split('_')[0][1:])): # sorted by stages\n",
    "            cmsList = comMotif_dict[m]\n",
    "            newEndIdx = newStartIdx + len(cmsList[0]) - 1\n",
    "            repNew.append((cmsList[0], newStartIdx, newEndIdx, cmsList[1], cmsList[2]))\n",
    "                      # [CMS, newCMSStartIdx, newCMSEndIdx, oriIdxRange1, oriIdxRange2]\n",
    "            newStartIdx = newEndIdx + 1\n",
    "\n",
    "    return repNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a dictionary that contains the initializing informations\n",
    "#\n",
    "# initialDict = {clusterName : (originalName, initialLength)}\n",
    "\n",
    "def getInitialDict(toMergeCandidateDict):\n",
    "    initialDict = dict()\n",
    "    for key, value in toMergeCandidateDict.items():\n",
    "        clusterName = value[0][0]\n",
    "        initialLen = value[0][1][0][2] + 1\n",
    "        originalName = value[1].pop()\n",
    "        initialDict[clusterName] = (originalName, initialLen)\n",
    "        value[1].add(originalName)\n",
    "    return initialDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a dict that contains only original name\n",
    "# nameDict = {clusterName: original name}\n",
    "\n",
    "def getInitialNameDict(initialDict):\n",
    "    nameDict = dict()\n",
    "    for key, value in initialDict.items():\n",
    "        name = value[0]\n",
    "        nameDict[key] = name\n",
    "    return nameDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# compute score of Rnew\n",
    "# the score calculate method is the length ratio of new to origin one\n",
    "\n",
    "# Ri is a tuple like ('G0', [[['A#A', 'C#C'], 0, 1, (0, 1), (1, 2)]])\n",
    "def compute_Score(Ri, Rj, Rnew):\n",
    "    if(Rnew[1]):\n",
    "        L_Ri = functools.reduce(lambda x,y:x+y, [(i[2]-i[1]+1) for i in Ri[1]])\n",
    "        L_Rj = functools.reduce(lambda x,y:x+y, [(j[2]-j[1]+1) for j in Rj[1]])\n",
    "    \n",
    "        Lorg = max(L_Ri, L_Rj)\n",
    "        Lnew = functools.reduce(lambda x,y:x+y, [(n[2]-n[1]+1) for n in Rnew[1]]) \n",
    "        return float(Lnew)/Lorg\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def snippetPacketChunking(toMergeCandidateDict, outputPath, roundCounter):\n",
    "#     if not os.path.isdir(outputPath): os.makedirs(outputPath)\n",
    "#     dictKeys = list(toMergeCandidateDict.keys())\n",
    "    \n",
    "#     snippetPacketDict = dict()\n",
    "#     snippetPacket = []\n",
    "\n",
    "#     print('Total Grams:',len(dictKeys))\n",
    "#     packetCount = 0\n",
    "#     pairCount = 0\n",
    "#     duplicateFlag = False\n",
    "#     for i in range(len(dictKeys)):\n",
    "#         for j in range(i+1, len(dictKeys)):\n",
    "#             pair = PairwisePair(toMergeCandidateDict[dictKeys[i]], toMergeCandidateDict[dictKeys[j]])\n",
    "#             snippetPacket.append(pair)\n",
    "#             pairCount += 1\n",
    "#             duplicateFlag = False\n",
    "#             if(len(snippetPacket) >= 10000):\n",
    "#                 snippetPacketDict[packetCount] = snippetPacket\n",
    "#                 snippetPacket = []\n",
    "#                 packetCount += 1\n",
    "#                 duplicateFlag = True # avoid loop end after leaving this if.\n",
    "\n",
    "#     if(not duplicateFlag):\n",
    "#         snippetPacketDict[packetCount] = snippetPacket\n",
    "\n",
    "#     print(\"Total Pairs:\", pairCount)\n",
    "#     print(\"Divided into \", len(snippetPacketDict.keys()), \"chunks\")\n",
    "    \n",
    "#     with open(outputPath + 'SnippetPacket_round'+str(roundCounter)+'.pickle', 'wb') as handle:\n",
    "#         pickle.dump(snippetPacketDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "# def findMergeCandidateScoreListOfSnippet(snippetPacket, generatedSeqNum, pickleDir, snippetIndex):\n",
    "#     if not os.path.isdir(pickleDir): os.makedirs(pickleDir)\n",
    "#     scoreList = [] #list()\n",
    "    \n",
    "#     for pair in snippetPacket:\n",
    "#         Ri = pair.getROfClusterI()\n",
    "#         Rj = pair.getROfClusterJ()\n",
    "#         repNew = get_Representative(Ri, Rj)\n",
    "#         clusterTempName = \"G\" + str(generatedSeqNum)\n",
    "#         Rnew = (clusterTempName , repNew)\n",
    "            \n",
    "#         # compute merge score of Rnew\n",
    "#         score = compute_Score(Ri, Rj, Rnew)\n",
    "#         Ri_name = Ri[0]\n",
    "#         Rj_name = Rj[0]\n",
    "#         scoreList.append((score, Rnew, Ri_name, Rj_name))\n",
    "#         Ri = None\n",
    "#         Rj = None\n",
    "#         repNew = None\n",
    "#         Rnew = None\n",
    "    \n",
    "#     print(\"ScoreList Len : \", len(scoreList))\n",
    "\n",
    "#     with open(pickleDir + str(snippetIndex) +\"tmp.pickle\", 'wb') as handle:\n",
    "#         pickle.dump(scoreList, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     scoreList = None\n",
    "        \n",
    "# def concateSnippetScoreList(tmpPickleDir):\n",
    "#     scoreList = []\n",
    "#     pList = os.listdir(tmpPickleDir)\n",
    "#     print(\"tmp Pickle numbers = \" , len(pList))\n",
    "#     for file in pList:\n",
    "#         with open(tmpPickleDir + file, 'rb') as inputFile:\n",
    "#             tmpList= pickle.load(inputFile)\n",
    "#             scoreList.extend(tmpList)\n",
    "#             print(\"TempList Len : \",len(tmpList))\n",
    "\n",
    "#     return sorted(scoreList, key=lambda tup:tup[0], reverse=True) # sorting by score (from biggest to smallest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score list of toMergeCandidateDict(single iteration) from highest to lowest\n",
    "\n",
    "def findMergeCandidateScoreList(toMergeCandidateDict, generatedSeqNum):\n",
    "    scoreList = [] #list()\n",
    "    dictKeys=[]\n",
    "#     del(list)\n",
    "    for k in toMergeCandidateDict:\n",
    "        dictKeys.append(k) #leoqaz12\n",
    "#     try:\n",
    "#         dictKeys = list(toMergeCandidateDict.keys()) #original\n",
    "#     except:\n",
    "#         del(list)\n",
    "#         dictKeys = list(toMergeCandidateDict.keys())\n",
    "    \n",
    "    sensitiveAPIs = {\"CreateProcessInternal\", \"OpenProcess\", \"WinExec\", \"CreateThread\", \"OpenThread\", \"CreateRemoteThread\",\n",
    "                     \"CopyFile\", \"CreateFile\", \"WriteFile\", \"ReadFile\", \"DeleteFile\", \"RegCreateKey\", \"RegSetValue\",\n",
    "                     \"InternetOpen\", \"InternetConnect\", \"HttpSendRequest\", \"WinHttpOpen\", \"WinHttpSendRequest\", \"WinHttpWriteData\", \"WinHttpCreateUrl\"}\n",
    "    \n",
    "    for i in range(len(dictKeys)):\n",
    "        for j in range(i+1, len(dictKeys)):\n",
    "            \n",
    "            # toMergeCandidateDict[i][1] is memberSet\n",
    "            Ri = toMergeCandidateDict[ dictKeys[i] ][0] # Ri is a tuple like ('G0', [[['A#A', 'C#C'], 0, 1, (0, 1), (1, 2)]])\n",
    "            Rj = toMergeCandidateDict[ dictKeys[j] ][0]\n",
    "#             print(toMergeCandidateDict[ dictKeys[i] ][1], toMergeCandidateDict[ dictKeys[j] ][1])\n",
    "            repNew = get_Representative(Ri, Rj)\n",
    "            clusterTempName = \"G\" + str(generatedSeqNum)\n",
    "            Rnew = (clusterTempName , repNew)\n",
    "\n",
    "            score = compute_Score(Ri, Rj, Rnew)\n",
    "            Ri_name = Ri[0]\n",
    "            Rj_name = Rj[0]\n",
    "            scoreList.append((score, Rnew, Ri_name, Rj_name))\n",
    "            Ri=None\n",
    "            Rj=None\n",
    "            Rnew=None\n",
    "            repNew=None\n",
    "#             else:\n",
    "#                 print(\"Rep Sequence Length smaller than 26! Length: \", RnewSequenceLen)\n",
    "\n",
    "    if(len(scoreList) > 0):\n",
    "        scoreList.sort(key=lambda tup:tup[0], reverse=True) # sorting by score (from biggest to smallest) \n",
    "#         print(\"ScoreList Length in method : \", len(scoreList))\n",
    "    else:\n",
    "        print(\"No common motif\")\n",
    "    \n",
    "    return scoreList # list = [(score, Rnew, Ri_name, Rj_name), (score, Rnew, Ri_name, Rj_name), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkExactlySameCandidates(scoreList):\n",
    "    globalPoolDict = dict() # a dict contains many sets.  dict = {index0: memberSet, 1: memberSet, 2:...}\n",
    "    newScoreList = [] #list() # list = [(score, R, memberSet), (score, R, memberSet), ...]\n",
    "    scoreListIdx = 0\n",
    "    for rank in scoreList:\n",
    "        score = rank[0]\n",
    "       \n",
    "        if(score == 1.0):\n",
    "            \n",
    "            Ri_name = rank[2]\n",
    "            Rj_name = rank[3]\n",
    "            \n",
    "            duplicate = False\n",
    "            for key, memberSet in globalPoolDict.items():\n",
    "                if(Ri_name in memberSet) or (Rj_name in memberSet):\n",
    "                    memberSet.add(Ri_name)\n",
    "                    memberSet.add(Rj_name)\n",
    "                    \n",
    "                    # update newScoreList 'memberSet' element\n",
    "                    newScoreList[key] = (newScoreList[key][0], newScoreList[key][1], memberSet)\n",
    "                    duplicate = True\n",
    "                    \n",
    "            # Find new independent pair, add into newScoreList and create new dict key\n",
    "            if(duplicate is False):\n",
    "                memberSet = set()\n",
    "                memberSet.add(Ri_name)\n",
    "                memberSet.add(Rj_name)\n",
    "                globalPoolDict[scoreListIdx] = memberSet\n",
    "                \n",
    "                Rnew = rank[1]\n",
    "                newScoreList.append((score, Rnew, memberSet))\n",
    "                scoreListIdx += 1\n",
    "        else:\n",
    "            Rnew = rank[1]\n",
    "            Ri_name = rank[2]\n",
    "            Rj_name = rank[3]\n",
    "            memberSet = set()\n",
    "            memberSet.add(Ri_name)\n",
    "            memberSet.add(Rj_name)\n",
    "            newScoreList.append((score, Rnew, memberSet))\n",
    "            scoreListIdx += 1\n",
    "    globalPoolDict = None\n",
    "    return newScoreList # list = [(score, R, memberSet), (score, R, memberSet), ...]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unit test\n",
    "# item1 = (1.0, (\"G0\", \"[['A#A', 'B#B','B#B', 'C#C','D#D'], 0, 2]\"), \"a.txt\", \"b.txt\")\n",
    "# item2 = (1.0, (\"G1\", \"[['A#A', 'B#B','B#B', 'C#C','D#D'], 0, 2]\"), \"a.txt\", \"c.txt\")\n",
    "# item3 = (1.0, (\"G2\", \"[['A#A', 'B#B','B#B', 'C#C','D#D'], 0, 2]\"), \"b.txt\", \"c.txt\")\n",
    "# item4 = (1.0, (\"G3\", \"[['A#A', 'B#B','B#B', 'C#C','D#D'], 0, 2]\"), \"c.txt\", \"d.txt\")\n",
    "# item5 = (1.0, (\"G4\", \"[['E#A', 'F#B'], 0, 2]\"), \"e.txt\", \"f.txt\")\n",
    "# item6 = (0.8, (\"G5\", \"[['X#A', 'Y#B'], 0, 2]\"), \"x.txt\", \"y.txt\")\n",
    "\n",
    "# scoreList = [item1, item2, item3, item4, item5, item6]\n",
    "\n",
    "# newScoreList = checkExactlySameCandidates(scoreList)\n",
    "# print(newScoreList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Rnew into toMergeCandidateDict and remove member of Rnew from candidates.\n",
    "\n",
    "def mergeCandidateClusters_new(toMergeCandidateDict, intermediatePoolDict, scoreList, generatedSeqNum, initialDict, definedThreshold):\n",
    "    initialNameDict = getInitialNameDict(initialDict) # get original name for reference in output.\n",
    "    \n",
    "    currentMergedSet = set()\n",
    "    for rank in scoreList:\n",
    "        score = rank[0]\n",
    "        memberSet = rank[2] # memberSet of highest score\n",
    "\n",
    "        # the minmum score this round is smaller than threshold\n",
    "        if(score < definedThreshold):\n",
    "            break\n",
    "        \n",
    "        exclusiveness = False\n",
    "        \n",
    "        # check exclusiveness\n",
    "        for member in memberSet:\n",
    "            if(member in currentMergedSet):\n",
    "                exclusiveness = True\n",
    "                break\n",
    "                \n",
    "        if(not exclusiveness):\n",
    "            clusterMembers = set() # create cluster member set with original Name\n",
    "            for member in memberSet:\n",
    "                nameOfMember = int(member.split('G')[1])\n",
    "                del toMergeCandidateDict[nameOfMember]\n",
    "                \n",
    "                if member in initialNameDict:\n",
    "                    clusterMembers.add(initialNameDict[member])\n",
    "                else:\n",
    "                    clusterMembers.add(member)\n",
    "                    \n",
    "                # Mark elements are merged\n",
    "                currentMergedSet.add(member) # update currentMergedSet\n",
    "            \n",
    "            Rnew = rank[1][1] # representative without old clusterName (i.e., rank[1] = (Name, Rep.))\n",
    "            newName = \"G\" + str(generatedSeqNum)\n",
    "            new_Cluster = (newName, Rnew)\n",
    "            \n",
    "            toMergeCandidateDict[generatedSeqNum] = (new_Cluster, clusterMembers)\n",
    "            intermediatePoolDict[generatedSeqNum] = (score, new_Cluster, clusterMembers) # (score, newCluster, members)\n",
    "            generatedSeqNum += 1\n",
    "    currentMergedSet = None\n",
    "    return toMergeCandidateDict, intermediatePoolDict, generatedSeqNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Rnew into toMergeCandidateDict and remove member of Rnew from candidates.\n",
    "\n",
    "def mergeCandidateClusters(toMergeCandidateDict, intermediatePoolDict, scoreList, generatedSeqNum, initialDict):\n",
    "    currentMergedSet = set()\n",
    "    \n",
    "    initialNameDict = getInitialNameDict(initialDict)\n",
    "    \n",
    "    for rank in scoreList:\n",
    "        Ri_name = rank[2] # member1 of highest score\n",
    "        Rj_name = rank[3] # member2 of highest score\n",
    "        \n",
    "        # check exclusiveness that candidate have been merged in current scoreList.\n",
    "        # if both two element haven't been processed then create new cluster.\n",
    "        if((Ri_name not in currentMergedSet) and (Rj_name not in currentMergedSet)):\n",
    "            # remove candidates in @toMergeCandidateDict\n",
    "            keyOfRi = int(Ri_name.split('G')[1])\n",
    "            keyOfRj = int(Rj_name.split('G')[1])\n",
    "            del toMergeCandidateDict[keyOfRi], toMergeCandidateDict[keyOfRj]\n",
    "\n",
    "            Rnew = rank[1] # get representative of highest score\n",
    "            newName = \"G\" + str(generatedSeqNum) # update clusterName\n",
    "        \n",
    "            new_Cluster = (newName, Rnew[1])\n",
    "\n",
    "            clusterMembers = set() # create cluster member set\n",
    "            if Ri_name in initialNameDict:\n",
    "                clusterMembers.add(initialNameDict[Ri_name])\n",
    "            else:\n",
    "                clusterMembers.add(Ri_name)\n",
    "            \n",
    "            \n",
    "            if Rj_name in initialNameDict:\n",
    "                clusterMembers.add(initialNameDict[Rj_name])\n",
    "            else:\n",
    "                clusterMembers.add(Rj_name)\n",
    "            \n",
    "            \n",
    "            toMergeCandidateDict[generatedSeqNum] = (new_Cluster, clusterMembers)\n",
    "            intermediatePoolDict[generatedSeqNum] = (rank[0], new_Cluster, clusterMembers) # (score, newCluster, members)\n",
    "\n",
    "            generatedSeqNum += 1\n",
    "        \n",
    "        # Mark elements are merged\n",
    "        currentMergedSet.add(Ri_name) # update currentMergedSet\n",
    "        currentMergedSet.add(Rj_name)\n",
    "        \n",
    "    return toMergeCandidateDict, intermediatePoolDict, generatedSeqNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterInitializedReps_semi(initializedReps_dict, tag, outputPath, thresholdValue):\n",
    "    intermediatePool = dict()\n",
    "    roundInfos = dict()\n",
    "    residual = None # used to save residual candidate when algorithm stop.\n",
    "\n",
    "    toMergeCandidateDict = initializedReps_dict # using residualRepsDict as toMergeCandidateDict (skip initialization)\n",
    "\n",
    "    # initialDict = {clusterName : (originalName, initialLength)}\n",
    "    initialDict = getInitialDict(toMergeCandidateDict)\n",
    "    \n",
    "    \n",
    "    roundProduct = [] #list()\n",
    "    for key, value in initialDict.items():\n",
    "        roundProduct.append(key)\n",
    "    roundInfos[0] = roundProduct # record product in round 0 (i.e., initialization)\n",
    "    \n",
    "#     generatedSeqNum = len(toMergeCandidateDict) # counter after initialize. Used to naming clusters.\n",
    "    generatedSeqNum = 818\n",
    "\n",
    "#     print(\"-- Start Clustering --\")\n",
    "#     print(\"Threshold set =\", thresholdValue)\n",
    "    roundCounter = 1\n",
    "    \n",
    "    lastBreakPoint = 0\n",
    "    \n",
    "    if(lastBreakPoint == 0):\n",
    "        with open(outputPath+'Pickle/' + 'tmpInit.pickle', 'wb') as f:\n",
    "            pickle.dump(initialDict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(outputPath+ \"pickle/\" + 'toMergeCandidate_round'+str(1)+'.pickle', 'wb') as handle:\n",
    "            pickle.dump(toMergeCandidateDict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        with open(outputPath+'Pickle/' + 'tmpInit.pickle', 'rb') as f:\n",
    "            initialDict = pickle.load(f)\n",
    "            \n",
    "    \n",
    "    while(1):\n",
    "#         print(\"Current Round : Round \", roundCounter)\n",
    "        if(roundCounter >= lastBreakPoint):\n",
    "\n",
    "            with open(outputPath+ \"pickle/\" + 'toMergeCandidate_round'+str(roundCounter)+'.pickle', 'rb') as mHandle:\n",
    "                toMergeCandidateDict = pickle.load(mHandle)\n",
    "            if(roundCounter != 1):\n",
    "                with open(outputPath+ \"pickle/\" + 'roundInfos_round'+str(roundCounter-1)+'.pickle', 'rb') as rHandle:\n",
    "                    roundInfos = pickle.load(rHandle)\n",
    "                with open(outputPath+ \"pickle/\" + 'intermediate_round'+str(roundCounter-1)+'.pickle', 'rb') as iHandle:\n",
    "                    intermediatePool = pickle.load(iHandle)\n",
    "\n",
    "\n",
    "            if(len(toMergeCandidateDict) == 1):\n",
    "                residual = toMergeCandidateDict # output residual candidates.\n",
    "                break\n",
    "\n",
    "            if(roundCounter != lastBreakPoint):\n",
    "                snippetPacketChunking(toMergeCandidateDict, outputPath+'SnippetPickle/', roundCounter)\n",
    "\n",
    "            with open(outputPath +'SnippetPickle/' + 'SnippetPacket_round'+str(roundCounter)+'.pickle', 'rb') as handle:\n",
    "                spDict = pickle.load(handle)\n",
    "\n",
    "            if(roundCounter == lastBreakPoint):\n",
    "                # calculate scoreList in candidate clusters\n",
    "                for snippetIndex in range(1, len(spDict.keys())):\n",
    "                    print('scoring snippet :' , snippetIndex)\n",
    "                    snippetPacket = spDict[snippetIndex]\n",
    "                    findMergeCandidateScoreListOfSnippet(snippetPacket,\n",
    "                                                         generatedSeqNum,\n",
    "                                                         outputPath+str(roundCounter)+'TmpPickle/',\n",
    "                                                         snippetIndex)\n",
    "            else:\n",
    "                for snippetIndex, snippetPacket in spDict.items():\n",
    "                    # if snippetIndex==5:break\n",
    "                    print('scoring snippet :' , snippetIndex)\n",
    "                    findMergeCandidateScoreListOfSnippet(snippetPacket,\n",
    "                                                         generatedSeqNum\n",
    "                                                         , outputPath+str(roundCounter)+'TmpPickle/',\n",
    "                                                         snippetIndex)\n",
    "            # break        \n",
    "            print(\"-- Finish scoring --\")\n",
    "            scoreList = concateSnippetScoreList(outputPath+str(roundCounter)+'TmpPickle/')\n",
    "            print(\"-- Finish concatenating scoreList --\")\n",
    "#             print(\"total ScoreList Length : \", len(scoreList))\n",
    "\n",
    "            # check and merge exactly the same candidates before merge clusters\n",
    "            scoreList = checkExactlySameCandidates(scoreList)\n",
    "            print(\"-- Finish checking 100% same candidates --\")\n",
    "            \n",
    "            with open(outputPath+str(roundCounter)+'TmpPickle/' + 'scoreList.pickle', 'wb') as sListHandle:\n",
    "                pickle.dump(scoreList, sListHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            # generated Clusters in This Round:\n",
    "            nameIdxStart = generatedSeqNum\n",
    "            \n",
    "            toMergeCandidateDict, intermediatePool, generatedSeqNum = mergeCandidateClusters_new(\n",
    "                toMergeCandidateDict, intermediatePool, scoreList, generatedSeqNum, initialDict, thresholdValue)\n",
    "            print(\"-- Finish merging clusters --\")\n",
    "\n",
    "#             print(\"generatedSeqNum now: \", generatedSeqNum)\n",
    "\n",
    "            # check if algorithm should stop when merge score under threshold\n",
    "            # if a score smaller than threshold, then it will break out when merging.\n",
    "            # Hense, if the 'generatedSeqNum' equals than 'nameIdxStart', means that no any new generated cluster.\n",
    "            # (if occurr a new cluster, generatedSeqNum will add one.)\n",
    "            if(generatedSeqNum == nameIdxStart):\n",
    "                residual = toMergeCandidateDict # output residual candidates.\n",
    "                break # end algorithm\n",
    "            \n",
    "            nameIdxEnd = generatedSeqNum\n",
    "            \n",
    "            # Record clusters generated in this round\n",
    "            for idx in range(nameIdxStart, nameIdxEnd):\n",
    "                if roundInfos.get(roundCounter) is None:\n",
    "                    roundProduct = [] #list()\n",
    "                    roundProduct.append(intermediatePool[idx][1][0])\n",
    "                    roundInfos[roundCounter] = roundProduct\n",
    "                else:\n",
    "                    roundInfos[roundCounter].append(intermediatePool[idx][1][0])\n",
    "                    \n",
    "            roundCounter += 1\n",
    "\n",
    "            with open(outputPath+ \"pickle/\" + 'toMergeCandidate_round'+str(roundCounter)+'.pickle', 'wb') as mHandle:\n",
    "                pickle.dump(toMergeCandidateDict, mHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(outputPath+ \"pickle/\" + 'intermediate_round'+str(roundCounter-1)+'.pickle', 'wb') as iHandle:\n",
    "                pickle.dump(intermediatePool, iHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(outputPath+ \"pickle/\" + 'roundInfos_round'+str(roundCounter-1)+'.pickle', 'wb') as rHandle:\n",
    "                pickle.dump(roundInfos, rHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "        else:\n",
    "            roundCounter += 1\n",
    "\n",
    "\n",
    "    print(\"-- Finish Clustering --\")\n",
    "\n",
    "    return intermediatePool, initialDict, roundInfos, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Main Function of SBBGCA ###\n",
    "\n",
    "import pickle\n",
    "\n",
    "def do_SBBGCA_clustering(data_directory, tag, outputPath, thresholdValue):\n",
    "    testDict = {0: (('G0', [[['A#A', 'B#B','B#B', 'C#C','D#D'], 0, 2]]),{\"a.trace.hooklog\"}),\n",
    "                1:(('G1', [[['A#A','B#B','C#C','D#D',\"G#G\"], 0, 2]]),{\"b.trace.hooklog\"}),\n",
    "                   2:(('G2', [[[\"B#B\",'F#F','C#C','D#D', 'G#G'], 0, 2]]),{\"c.trace.hooklog\"}),\n",
    "                      3:(('G3', [[['Q#Q','C#C','D#D','G#G','M#M'], 0, 2]]),{\"d.trace.hooklog\"}),\n",
    "                           4:(('G4', [[['A#A','Q#Q','C#C','G#G','M#M'], 0, 2]]),{\"e.trace.hooklog\"})}\n",
    "    intermediatePool = dict()\n",
    "    roundInfos = dict()\n",
    "    residual = None # used to save residual candidate when algorithm stop.\n",
    "#     toMergeCandidateDict = testDict\n",
    "    toMergeCandidateDict = initialCandidateDict(data_directory) # initialize @toMergeCandidateDict\n",
    "\n",
    "    # initialDict = {clusterName : (originalName, initialLength)}\n",
    "    initialDict = getInitialDict(toMergeCandidateDict)\n",
    "    \n",
    "    roundProduct = [] #list()\n",
    "    for key, value in initialDict.items():\n",
    "        roundProduct.append(key)\n",
    "    roundInfos[0] = roundProduct # record product in round 0 (i.e., initialization)\n",
    "    \n",
    "    generatedSeqNum = len(toMergeCandidateDict) # counter after initialize. Used to naming clusters.\n",
    "\n",
    "#     print(\"-- Start Clustering --\")\n",
    "#     print(\"Threshold set =\", thresholdValue)\n",
    "    roundCounter = 1\n",
    "#     generatedSeqNum = 224\n",
    "#     roundCounter = 3\n",
    "    try:\n",
    "        if(roundCounter != 1):\n",
    "            with open(outputPath+ \"pickle/\" + 'toMergeCandidate_round'+str(roundCounter)+'.pickle', 'rb') as mHandle:\n",
    "                toMergeCandidateDict = pickle.load(mHandle)\n",
    "            with open(outputPath+ \"pickle/\" + 'roundInfos_round'+str(roundCounter-1)+'.pickle', 'rb') as rHandle:\n",
    "                roundInfos = pickle.load(rHandle)\n",
    "            with open(outputPath+ \"pickle/\" + 'intermediate_round'+str(roundCounter-1)+'.pickle', 'rb') as iHandle:\n",
    "                intermediatePool = pickle.load(iHandle)\n",
    "\n",
    "        while(1):\n",
    "            print(\"Round: \", roundCounter)\n",
    "            if(len(toMergeCandidateDict) == 1):\n",
    "                residual = toMergeCandidateDict # output residual candidates.\n",
    "                break\n",
    "\n",
    "            # calculate scoreList in candidate clusters\n",
    "            scoreList = findMergeCandidateScoreList(toMergeCandidateDict, generatedSeqNum)\n",
    "\n",
    "            # check and merge exactly the same candidates before merge clusters\n",
    "            scoreList = checkExactlySameCandidates(scoreList)\n",
    "\n",
    "            import os\n",
    "            if not os.path.isdir(outputPath+str(roundCounter)+'TmpPickle/'):\n",
    "                os.makedirs(outputPath+str(roundCounter)+'TmpPickle/')\n",
    "\n",
    "            with open(outputPath+str(roundCounter)+'TmpPickle/' + 'scoreList.pickle', 'wb') as sListHandle:\n",
    "                    pickle.dump(scoreList, sListHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            # generated Clusters in This Round:\n",
    "            nameIdxStart = generatedSeqNum\n",
    "\n",
    "            toMergeCandidateDict, intermediatePool, generatedSeqNum = mergeCandidateClusters_new(\n",
    "                toMergeCandidateDict, intermediatePool, scoreList, generatedSeqNum, initialDict, thresholdValue)\n",
    "\n",
    "            print(\"generatedSeqNum now: \", generatedSeqNum)\n",
    "\n",
    "            # check if algorithm should stop when merge score under threshold\n",
    "            # if a score smaller than threshold, then it will break out when merging.\n",
    "            # Hense, if the 'generatedSeqNum' equals than 'nameIdxStart', means that no any new generated cluster.\n",
    "            # (if occurr a new cluster, generatedSeqNum will add one.)\n",
    "            if(generatedSeqNum == nameIdxStart):\n",
    "                residual = toMergeCandidateDict # output residual candidates.\n",
    "                break # end algorithm\n",
    "\n",
    "            nameIdxEnd = generatedSeqNum\n",
    "\n",
    "            # Record clusters generated in this round\n",
    "            for idx in range(nameIdxStart, nameIdxEnd):\n",
    "                if roundInfos.get(roundCounter) is None:\n",
    "                    roundProduct = [] #list()\n",
    "                    roundProduct.append(intermediatePool[idx][1][0])\n",
    "                    roundInfos[roundCounter] = roundProduct\n",
    "                else:\n",
    "                    roundInfos[roundCounter].append(intermediatePool[idx][1][0])\n",
    "\n",
    "            roundCounter += 1\n",
    "\n",
    "            with open(outputPath+ \"pickle/\" + 'toMergeCandidate_round'+str(roundCounter)+'.pickle', 'wb') as mHandle:\n",
    "                pickle.dump(toMergeCandidateDict, mHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(outputPath+ \"pickle/\" + 'intermediate_round'+str(roundCounter-1)+'.pickle', 'wb') as iHandle:\n",
    "                pickle.dump(intermediatePool, iHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(outputPath+ \"pickle/\" + 'roundInfos_round'+str(roundCounter-1)+'.pickle', 'wb') as rHandle:\n",
    "                pickle.dump(roundInfos, rHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#         print(\"-- Finish Clustering --\")\n",
    "    except:\n",
    "        generatedSeqNum = generatedSeqNum\n",
    "        roundCounter = roundCounter\n",
    "        if(roundCounter != 1):\n",
    "            with open(outputPath+ \"pickle/\" + 'toMergeCandidate_round'+str(roundCounter)+'.pickle', 'rb') as mHandle:\n",
    "                toMergeCandidateDict = pickle.load(mHandle)\n",
    "            with open(outputPath+ \"pickle/\" + 'roundInfos_round'+str(roundCounter-1)+'.pickle', 'rb') as rHandle:\n",
    "                roundInfos = pickle.load(rHandle)\n",
    "            with open(outputPath+ \"pickle/\" + 'intermediate_round'+str(roundCounter-1)+'.pickle', 'rb') as iHandle:\n",
    "                intermediatePool = pickle.load(iHandle)\n",
    "\n",
    "        while(1):\n",
    "            print(\"Round: \", roundCounter)\n",
    "            if(len(toMergeCandidateDict) == 1):\n",
    "                residual = toMergeCandidateDict # output residual candidates.\n",
    "                break\n",
    "\n",
    "            # calculate scoreList in candidate clusters\n",
    "            scoreList = findMergeCandidateScoreList(toMergeCandidateDict, generatedSeqNum)\n",
    "\n",
    "            # check and merge exactly the same candidates before merge clusters\n",
    "            scoreList = checkExactlySameCandidates(scoreList)\n",
    "\n",
    "            import os\n",
    "            if not os.path.isdir(outputPath+str(roundCounter)+'TmpPickle/'):\n",
    "                os.makedirs(outputPath+str(roundCounter)+'TmpPickle/')\n",
    "\n",
    "            with open(outputPath+str(roundCounter)+'TmpPickle/' + 'scoreList.pickle', 'wb') as sListHandle:\n",
    "                    pickle.dump(scoreList, sListHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            # generated Clusters in This Round:\n",
    "            nameIdxStart = generatedSeqNum\n",
    "\n",
    "            toMergeCandidateDict, intermediatePool, generatedSeqNum = mergeCandidateClusters_new(\n",
    "                toMergeCandidateDict, intermediatePool, scoreList, generatedSeqNum, initialDict, thresholdValue)\n",
    "\n",
    "            print(\"generatedSeqNum now: \", generatedSeqNum)\n",
    "\n",
    "            # check if algorithm should stop when merge score under threshold\n",
    "            # if a score smaller than threshold, then it will break out when merging.\n",
    "            # Hense, if the 'generatedSeqNum' equals than 'nameIdxStart', means that no any new generated cluster.\n",
    "            # (if occurr a new cluster, generatedSeqNum will add one.)\n",
    "            if(generatedSeqNum == nameIdxStart):\n",
    "                residual = toMergeCandidateDict # output residual candidates.\n",
    "                break # end algorithm\n",
    "\n",
    "            nameIdxEnd = generatedSeqNum\n",
    "\n",
    "            # Record clusters generated in this round\n",
    "            for idx in range(nameIdxStart, nameIdxEnd):\n",
    "                if roundInfos.get(roundCounter) is None:\n",
    "                    roundProduct = [] #list()\n",
    "                    roundProduct.append(intermediatePool[idx][1][0])\n",
    "                    roundInfos[roundCounter] = roundProduct\n",
    "                else:\n",
    "                    roundInfos[roundCounter].append(intermediatePool[idx][1][0])\n",
    "\n",
    "            roundCounter += 1\n",
    "\n",
    "            with open(outputPath+ \"pickle/\" + 'toMergeCandidate_round'+str(roundCounter)+'.pickle', 'wb') as mHandle:\n",
    "                pickle.dump(toMergeCandidateDict, mHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(outputPath+ \"pickle/\" + 'intermediate_round'+str(roundCounter-1)+'.pickle', 'wb') as iHandle:\n",
    "                pickle.dump(intermediatePool, iHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(outputPath+ \"pickle/\" + 'roundInfos_round'+str(roundCounter-1)+'.pickle', 'wb') as rHandle:\n",
    "                pickle.dump(roundInfos, rHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#         print(\"-- Finish Clustering --\")\n",
    "    return intermediatePool, initialDict, roundInfos, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterInitializedReps(initializedReps_dict, tag, outputPath, thresholdValue):\n",
    "    intermediatePool = dict()\n",
    "    roundInfos = dict()\n",
    "    residual = None # used to save residual candidate when algorithm stop.\n",
    "#     toMergeCandidateDict = testDict\n",
    "    toMergeCandidateDict = initializedReps_dict # using residualRepsDict as toMergeCandidateDict (skip initialization)\n",
    "\n",
    "    # initialDict = {clusterName : (originalName, initialLength)}\n",
    "    initialDict = getInitialDict(toMergeCandidateDict)\n",
    "#     with open(outputPath+'Pickle/' + 'tmpInit.pickle', 'wb') as sListHandle:\n",
    "#             pickle.dump(initialDict, sListHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    roundProduct = [] #list()\n",
    "    for key, value in initialDict.items():\n",
    "        roundProduct.append(key)\n",
    "    roundInfos[0] = roundProduct # record product in round 0 (i.e., initialization)\n",
    "    \n",
    "    generatedSeqNum = len(toMergeCandidateDict) # counter after initialize. Used to naming clusters.\n",
    "\n",
    "#     print(\"-- Start Clustering --\")\n",
    "#     print(\"Threshold set =\", thresholdValue)\n",
    "    roundCounter = 1\n",
    "    \n",
    "    while(1):\n",
    "#         print(\"Current Round : Round \", roundCounter)\n",
    "        if(len(toMergeCandidateDict) == 1):\n",
    "            residual = toMergeCandidateDict # output residual candidates.\n",
    "            break\n",
    "\n",
    "        # calculate scoreList in candidate clusters\n",
    "        scoreList = findMergeCandidateScoreList(toMergeCandidateDict, generatedSeqNum)\n",
    "        print(\"-- Finish scoring --\")\n",
    "#         print(\"ScoreList Len : \", len(scoreList))\n",
    "        \n",
    "        # check and merge exactly the same candidates before merge clusters\n",
    "        scoreList = checkExactlySameCandidates(scoreList)\n",
    "        print(\"-- Finish checking 100% same candidates --\")\n",
    "       \n",
    "        if not os.path.isdir(outputPath+str(roundCounter)+'Pickle/'): os.makedirs(outputPath+str(roundCounter)+'Pickle/')\n",
    "        with open(outputPath+str(roundCounter)+'Pickle/' + 'scoreList.pickle', 'wb') as sListHandle:\n",
    "            pickle.dump(scoreList, sListHandle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        # generated Clusters in This Round:\n",
    "        nameIdxStart = generatedSeqNum\n",
    "        \n",
    "        toMergeCandidateDict, intermediatePool, generatedSeqNum = mergeCandidateClusters_new(\n",
    "            toMergeCandidateDict, intermediatePool, scoreList, generatedSeqNum, initialDict, thresholdValue)\n",
    "        print(\"-- Finish merging clusters --\")\n",
    "        # check if algorithm should stop when merge score under threshold\n",
    "        # if a score smaller than threshold, then it will break out when merging.\n",
    "        # Hense, if the 'generatedSeqNum' equals than 'nameIdxStart', means that no any new generated cluster.\n",
    "        # (if occurr a new cluster, generatedSeqNum will add one.)\n",
    "        if(generatedSeqNum == nameIdxStart):\n",
    "            residual = toMergeCandidateDict # output residual candidates.\n",
    "            break # end algorithm\n",
    "        \n",
    "        nameIdxEnd = generatedSeqNum\n",
    "        \n",
    "        # Record clusters generated in this round\n",
    "        for idx in range(nameIdxStart, nameIdxEnd):\n",
    "            if roundInfos.get(roundCounter) is None:\n",
    "                roundProduct = [] #list()\n",
    "                roundProduct.append(intermediatePool[idx][1][0])\n",
    "                roundInfos[roundCounter] = roundProduct\n",
    "            else:\n",
    "                roundInfos[roundCounter].append(intermediatePool[idx][1][0])\n",
    "                \n",
    "        roundCounter += 1\n",
    "    print(\"-- Finish Clustering --\")\n",
    "\n",
    "    return intermediatePool, initialDict, roundInfos, residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import threading\n",
    "# import time\n",
    "# from threading import Thread\n",
    "\n",
    "\n",
    "# exitFlag = 0\n",
    "\n",
    "# class myThread (threading.Thread):\n",
    "#     def __init__(self, threadID, name, counter):\n",
    "#         threading.Thread.__init__(self)\n",
    "#         self.threadID = threadID\n",
    "#         self.name = name\n",
    "#         self.counter = counter\n",
    "#     def run(self):\n",
    "#         print (\"Starting \" + self.name)\n",
    "#         print_time(self.name, 1, self.counter)\n",
    "#         print (\"Exiting \" + self.name)\n",
    "# #         return -1\n",
    "#     def join(self):\n",
    "#         Thread.join(self)\n",
    "#         return -1\n",
    "\n",
    "# def print_time(threadName, counter, delay):\n",
    "#     while counter:\n",
    "# #         if exitFlag:\n",
    "# #             threadName.exit()\n",
    "#         time.sleep(delay)\n",
    "#         print (\"%s: %s\" % (threadName, time.ctime(time.time())))\n",
    "#         counter -= 1\n",
    "# gg= [1,2,3,4,5]\n",
    "# kk=['oo','ff','kk','yy','zz','11','22','44','666','1111','ggg','gjj','qqq','pppp','sss','oooo']\n",
    "# # Create new threads\n",
    "# c=0\n",
    "# for v in kk:\n",
    "#     for vv in gg:\n",
    "#         thread1 = myThread(1, v, vv)\n",
    "#     #     thread2 = myThread(2, v, 1)\n",
    "#         k1= thread1.start()\n",
    "#     #     thread2.start()\n",
    "#         c+=1\n",
    "#         c+\n",
    "#         if c==10:\n",
    "#             thread1.join()\n",
    "#             c=0\n",
    "#         time.sleep(1)\n",
    "\n",
    "# # Start new Threads\n",
    "\n",
    "\n",
    "# print (\"Exiting Main Thread\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
